---
title: "Exploring the p > n problem"
subtitle: "STAT 494: Statistical Genetics"
author: "Solutions"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part 1: Toy Data Example

To understand the *p > n problem*, we'll start by exploring a toy data example.


## One Variant

To simulate a biallelic genetic variant, we can use the `rbinom` function in R. This will **r**andomly generate data from a **binom**ial distribution. If we set $n$ equal to 2 and $p$ equal to our minor allele frequency (MAF), the `rbinom` function will randomly generate counts between 0, 1, and 2 that we can think of as representing the number of minor alleles (0, 1, or 2) carried by each individual at that position.

Run the code below to see this in action:

```{r simulate-one-variant}
set.seed(494) # for reproducible random number generation
snp <- rbinom(n = 100, size = 2, p = 0.1) # 100 people, MAF = 0.1
print(snp)
```

Try changing $p$ or $n$ to see how the data change with different minor allele frequencies and sample sizes.



## Many Variants

In a typical GWAS, we're working with a lot more than just a single variant. If we want to quickly generate many variants, it can be useful to first create our own function that will generate one variant (`do_one`) and then use `replicate` to run that function many times, thus generating many variants. In reality, nearby genetic variants are correlated with one another, but we'll ignore that correlation stucture for now and just generate 1000 independent variants.

```{r simulate-many-variants}
# write a function to simulate one genetic variant
do_one <- function(n_ppl, MAF){
  snp <- rbinom(n = n_ppl, size = 2, p = MAF)
  return(snp)
}

# replicate 1000 times to create 1000 variants
set.seed(494)
snps <- replicate(1000, do_one(n_ppl = 100, MAF = 0.1))
```

What is the dimension of the `snps` object that we just created? Is this an example of "big" data?

> The `snps` object has `r nrow(snps)` rows ($n$) and `r ncol(snps)` columns ($p$), so yes this is an example of big data ($p > n$)!

```{r}
dim(snps)
```


## Simulated Trait

Our last step in creating our toy dataset is to add a trait. Let's create a *null* trait (that isn't associated with any genetic variants) using the `rnorm` function to randomly sample from a normal distribution: 

```{r simulate-null-trait}
y <- rnorm(100, mean = 65, sd = 3)
print(y)
```

Finally, we can combine together our simulated SNPs and trait into a single data frame: 

```{r combine-into-dataset}
dat <- as.data.frame(cbind(snps, y))
```



## Multiple Linear Regression

Try fitting a multiple linear regression model to investigate the association between our trait, $y$, and our 1000 SNPs. *Hint: instead of typing out the name of each SNP variable (`lm(y ~ V1 + V2 + V3 + ...`), use the shortcut `lm(y ~ .)` to tell R to use all of the predictor variables).*

```{r attempt-multiple-regression}
lm(y ~ ., data = dat)
```

What happens when you try to fit this multiple linear regression model?

> R is able to estimate the first 100 coefficients (the intercept through the slope for the 99th variant), but all coefficients after that are reported as "NA". 
> 
> Notice that if we change one of the `lm` arguments (open the help file for `lm` to read more about this), we see the following:

```{r, error = T}
lm(y ~ ., data = dat, singular.ok = FALSE)
```

# Part 2: Theory of Linear Models

To understand what's happening here, we need to review some concepts from Calculus, Linear Algebra, and STAT 155.


## Definition: Method of Least Squares

To estimate the coefficients in a multiple linear regression model, we typically use an approach known as *least squares estimation.*

In fitting a linear regression model, our goal is to find the line that provides the "best" fit to our data. More specifically, we hope to find the values of the intercept $\beta_0$ and slopes $\beta_1, \beta_2, \dots$ that **minimize** the sum of **squared** residuals:

$$\sum_{i=1}^n r_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - [\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots])^2$$

In other words, we're trying to find the coefficient values that minimize the sum of squares of the vertical distances from the observed data points $(\mathbf{x}_i, y_i)$ to the fitted values $(\mathbf{x}_i, \hat{y}_i)$.

Take a look at the [Stat 155 Notes](https://bcheggeseth.github.io/Stat155Notes/least-squares.html) if you need/want to review this concept.



## Useful Techniques: Minimizing Functions

In order to find the least squares estimates for the intercept and slope(s) of our linear regression model, we need to *minimize* a function---the sum of squared residuals. To do this, we can use techniques from multivariate calculus! 

Remember from calculus that if we want to find the value of $x$ that minimizes the function $f(x)$, then we need to:

1. take the derivative $f'(x) = \frac{d}{dx} f(x)$
2. set the derivative equal to zero and solve for $x$
3. (technically, we should then also find the second derivative $f''(x)$, evaluate at the solution to step #2, and check if it's positive in order to confirm we have a minimum)


## Example: Simple Linear Regression

To find the least squares estimates $\hat\beta_0, \hat\beta_1$ for a simple linear regression model (with one predictor variable) $$E[y \mid x] = \beta_0 + \beta_1 x,$$ start by finding the partial derivatives $\frac{\partial}{\partial\beta_0}$ and $\frac{\partial}{\partial\beta_1}$ of the sum of squared residuals:

$$
\begin{aligned}
\frac{\partial}{\partial\beta_0} \sum_{i=1}^n (y_i - [\beta_0 + \beta_1 x_i])^2 
&= \sum_{i=1}^n 2(y_i - [\beta_0 + \beta_1 x_i])(-1) \\ 
& = \sum_{i=1}^n -2y_i + \sum_{i=1}^n 2\beta_0 + \sum_{i=1}^n 2\beta_1 x_i \\
&= -2 n \bar{y} + 2n \beta_0 + 2 \beta_1 n \bar{x}, \text{ where } \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \text{ and } \bar{x} = \sum_{i=1}^n x_i   \\ 
&= 2n(\beta_0 + \beta_1 \bar{x} - \bar{y})\\
\end{aligned}
$$

Now you try: find the partial derivative with respect to $\beta_1$.

$$
\begin{aligned}
\frac{\partial}{\partial\beta_1} \sum_{i=1}^n (y_i - [\beta_0 + \beta_1 x_i])^2 
&= \sum_{i=1}^n 2(y_i - [\beta_0 + \beta_1 x_i])(-x_i) \\ 
&= -2 \sum_{i=1}^n y_ix_i + 2\beta_0 \sum_{i=1}^n x_i + 2\beta_1 \sum_{i=1}^n x_i^2 \\
&= -2 \sum_{i=1}^n y_i x_i + 2\beta_0 n\bar{x} + 2 \beta_1 \sum_{i=1}^n x_i^2 \\
& = 2(n\beta_0 \bar{x} + \beta_1 \sum_{i=1}^n x_i^2 -\sum_{i=1}^n y_i x_i)\\
\end{aligned}
$$


Then, set both of these partial derivatives equal to zero and solve for $\beta_0, \beta_1$: 

$$
\begin{aligned}
\frac{\partial}{\partial\beta_0} \sum_{i=1}^n (y_i - [\beta_0 + \beta_1 x_i])^2 &\stackrel{set}{=} 0 \\
2n(\beta_0 + \beta_1 \bar{x} - \bar{y}) & = 0, \text{ plugging in the partial derivative from above} \\
\beta_0 + \beta_1 \bar{x} - \bar{y} & = 0 \\
\beta_0 &= \bar{y} - \beta_1 \bar{x}
\end{aligned}
$$

> This is as far as we can simplify for now. Let's switch to looking at the other partial derivative:

$$
\begin{aligned}
\frac{\partial}{\partial\beta_1} \sum_{i=1}^n (y_i - [\beta_0 + \beta_1 x_i])^2 &\stackrel{set}{=} 0 \\
2(n\beta_0 \bar{x} + \beta_1 \sum_{i=1}^n x_i^2 -\sum_{i=1}^n y_i x_i) &= 0, \text{ plugging in the partial derivative from above} \\
n\beta_0 \bar{x} + \beta_1 \sum_{i=1}^n x_i^2 -\sum_{i=1}^n y_i x_i &= 0 \\
n(\bar{y} - \beta_1 \bar{x}) \bar{x} + \beta_1 \sum_{i=1}^n x_i^2 -\sum_{i=1}^n y_i x_i &= 0, \text{ plugging in } \beta_0 = \bar{y} - \beta_1 \bar{x} \text{ from above} \\
n\bar{y}\bar{x} - n\beta_1 \bar{x}^2 + \beta_1 \sum_{i=1}^n x_i^2 -\sum_{i=1}^n y_i x_i &= 0 \\
n\bar{y}\bar{x} + \beta_1\left(\sum_{i=1}^n x_i^2 - n\bar{x}^2\right) -\sum_{i=1}^n y_i x_i &= 0 \\
\beta_1\left(\sum_{i=1}^n x_i^2 - n\bar{x}^2\right) &= \sum_{i=1}^n y_i x_i - n\bar{y}\bar{x} \\
\implies \hat\beta_1 & = \frac{\sum_{i=1}^n y_i x_i - n\bar{y}\bar{x}}{\sum_{i=1}^n x_i^2 - n\bar{x}^2} \\
\end{aligned}
$$

> From here, we can plug our slope estimate $\hat\beta_1$ back into the equation for $\beta_0$ above to get our estimate for the intercept 

$$\hat\beta_0 = \bar{y} - \hat\beta_1 \bar{x}.$$

Check your answer: if you did everything correctly, your estimates should match the equations in the [Stat 155 Notes](https://bcheggeseth.github.io/Stat155Notes/least-squares.html). 

> Note that $\sum_{i=1}^n y_i x_i - n \bar{y}\bar{x} = \sum_{i=1}^n (y_i - \bar{y}) (x_i - \bar{x})$:

$$
\begin{aligned}
\sum_{i=1}^n (y_i - \bar{y}) (x_i - \bar{x}) & = \sum_{i=1}^n (y_i x_i - y_i \bar{x} - \bar{y}x_i + \bar{y}\bar{x}) \\
& = \sum_{i=1}^n y_i x_i - \sum_{i=1}^n y_i \bar{x} - \sum_{i=1}^n \bar{y} x_i + \sum_{i=1}^n \bar{y} \bar{x} \\
& = \sum_{i=1}^n y_i x_i - \bar{x}\sum_{i=1}^n y_i  - \bar{y}\sum_{i=1}^n  x_i + n \bar{y} \bar{x} \\
& =  \sum_{i=1}^n y_i x_i - \bar{x}(n \bar{y})  - \bar{y}(n \bar{x}) + n \bar{y} \bar{x} \\
& =  \sum_{i=1}^n y_i x_i - n \bar{y}\bar{x}
\end{aligned}
$$

> We can use a similar approach to show that $\sum_{i=1}^n x_i^2 - n \bar{x}^2 = \sum_{i=1}^n (x_i - \bar{x})^2$.

## Matrix Version of Least Squares

Deriving the least squares and maximum likelihood estimates for $\beta_0$ and $\beta_1$ was perhaps a bit tedious, but do-able, for a simple linear regression model with one explanatory variable. Things get more difficult when we consider multiple linear regression models with many explanatory variables:

$$E[y | x_1, x_2, ..., x_p] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots \beta_p x_p.$$

When we have many explanatory variables, it's often useful to formulate our linear regression model using vectors and matrices. 

Consider the vector of outcomes $\mathbf{y}$

$$\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix},$$

the vector of covariates $\boldsymbol\beta$

$$\boldsymbol\beta = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix},$$

and the matrix of covariates (sometimes referred to as the "design matrix") $\mathbf{X}$

$$\mathbf{X} = \begin{pmatrix} 1 & x_{11} & \cdots & x_{p1} \\ 1 & x_{12} & \cdots & x_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & \cdots & x_{pn} \end{pmatrix}.$$

Then, we can write our linear regression model as 

$$E[\mathbf{y} \mid \mathbf{X}] = \boldsymbol\beta \mathbf{X}$$ 

or 

$$\mathbf{y} = \mathbf{X}\boldsymbol\beta  + \boldsymbol\epsilon,$$ 

where $E[\boldsymbol\epsilon] = \mathbf{0}$.

> Remember that matrix algebra multiplication like this:

$$
\begin{aligned}
\mathbf{X}\boldsymbol\beta & = \begin{pmatrix} 1 & x_{11} & \cdots & x_{p1} \\ 1 & x_{12} & \cdots & x_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & \cdots & x_{pn} \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix} \\
& = \begin{pmatrix} 1 \times \beta_0 + x_{11}\times\beta_1 + \dots + x_{p1}\times\beta_p \\
1 \times \beta_0 + x_{12}\times\beta_1 + \dots + x_{p2}\times\beta_p \\
\vdots \\
1 \times \beta_0 + x_{1n}\times\beta_1 + \dots + x_{pn}\times\beta_p \\
\end{pmatrix} \\
& = \begin{pmatrix} \beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1} \\
\beta_0 + \beta_1 x_{12} + \dots + \beta_p x_{p2} \\
\vdots \\
\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn} \\
\end{pmatrix} \\
\end{aligned}
$$

Using matrix notation, we can formulate the least squares problem as follows: 

$$\text{argmin}_{\boldsymbol\beta} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} - \mathbf{X}\boldsymbol\beta),$$

where the notation $\text{argmin}_{\boldsymbol\beta} f(\boldsymbol\beta)$ means that we want to find the value of $\boldsymbol\beta$ that minimizes the function $f(\boldsymbol\beta)$.

> Let's convince ourselves that this is equivalent to the sum of squared residuals $\sum_{i=1}^n r_i^2$ as we're used to seeing it:

$$
\begin{aligned}
(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} - \mathbf{X}\boldsymbol\beta) & = \left(\begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} -\begin{pmatrix} \beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1} \\
\vdots \\
\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn} \\
\end{pmatrix}   \right)^\top \left(\begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} -\begin{pmatrix} \beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1} \\
\vdots \\
\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn} \\
\end{pmatrix}   \right) \\
& = \begin{pmatrix} y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1}) \\
\vdots \\
y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) \\
\end{pmatrix} ^\top \begin{pmatrix} y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1}) \\
\vdots \\
y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) \\
\end{pmatrix}  \\
& = \begin{pmatrix} y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1}) &
\cdots &
y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) \\
\end{pmatrix} \begin{pmatrix} y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1}) \\
\vdots \\
y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) \\
\end{pmatrix}  \\
& = [y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1})] [y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1})] + \cdots + [y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) ] [y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) ] \\
& = [y_1 - (\beta_0 + \beta_1 x_{11} + \dots + \beta_p x_{p1})]^2 + \cdots + [y_n - (\beta_0 + \beta_1 x_{1n} + \dots + \beta_p x_{pn}) ]^2 \\
& = \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi})]^2 
\end{aligned}
$$

## Matrix Calculus

Find the value of $\boldsymbol\beta$ that minimizes the sum of squared residuals $(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} - \mathbf{X}\boldsymbol\beta)$.

*Hint: review these results about  [vectors](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) and [matrices](https://drive.google.com/file/d/1SnpeNAP75ufBIgvfmb68PxLLP9mc-wyg/view?usp=sharing) if needed.*

> We'll use the following results in particular: 
>
> - $(a + b)^\top c = a^\top c + b^\top c$
> - $(AB)^\top = B^\top A^\top$
> - $a^\top b = b^\top a$
> - $\frac{\partial}{\partial \mathbf{a}} \mathbf{a}^\top \mathbf{b} = \mathbf{b} = \frac{\partial}{\partial \mathbf{a}} \mathbf{b}^\top \mathbf{a}$
> - $\frac{\partial}{\partial \mathbf{a}} \mathbf{a}^\top \mathbf{B} \mathbf{a} = 2\mathbf{B}\mathbf{a}$

First, take the derivative with respect to $\boldsymbol\beta$: 

$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol\beta} (\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} - \mathbf{X}\boldsymbol\beta) 
& = \frac{\partial}{\partial \boldsymbol\beta} \left(\mathbf{y}^\top \mathbf{y} - (\mathbf{X}\boldsymbol\beta)^\top\mathbf{y} - \mathbf{y}^\top (\mathbf{X}\boldsymbol\beta) + (\mathbf{X}\boldsymbol\beta)^\top (\mathbf{X}\boldsymbol\beta) \right)\\
& = \frac{\partial}{\partial \boldsymbol\beta} \left(\mathbf{y}^\top \mathbf{y} - 2\mathbf{y}^\top\mathbf{X}\boldsymbol\beta   + \boldsymbol\beta^\top \mathbf{X}^\top \mathbf{X} \boldsymbol\beta \right) \\
& = -2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top \mathbf{X} \boldsymbol\beta
\end{aligned}
$$


Then, set this equal to zero and solve for $\boldsymbol\beta$:

$$
\begin{aligned}
-2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top \mathbf{X} \boldsymbol\beta &\stackrel{set}{=} 0 \\
2 \mathbf{X}^\top \mathbf{X} \boldsymbol\beta & = 2\mathbf{X}^\top\mathbf{y} \\
\mathbf{X}^\top \mathbf{X} \boldsymbol\beta  & = \mathbf{X}^\top\mathbf{y} \\
(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X} \boldsymbol\beta   & = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} \\
\hat{\boldsymbol\beta} & = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} 
\end{aligned}
$$

Check your answer: you should end up with the solution $\hat{\boldsymbol\beta} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$.



## The p > n problem, revisited

Now that we've done all that math, we can finally come back to our p > n problem. 

We saw above that the least squares estimator for the coefficients of a multiple linear regression model can be written as $$\hat{\boldsymbol\beta} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}.$$

Let's calculate $\mathbf{X}^\top \mathbf{X}$ for our toy example, above:

```{r}
xtx <- t(snps) %*% snps
```


What is the determinant of $\mathbf{X}^\top \mathbf{X}$?

```{r calculate-determinant}
det(xtx)
```

> The determinant is zero!

What does this tell us about $(\mathbf{X}^\top \mathbf{X})^{-1}$?

> Since the determinant of $\mathbf{X}^\top \mathbf{X}$ is zero, this tell us that it is not invertible. In other words, $(\mathbf{X}^\top \mathbf{X})^{-1}$ does not exist, and thus our least squares estimator $$\hat{\boldsymbol\beta} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$$ is not defined.

> Another way to think about this: since $\mathbf{X}$ has more columns than rows, the columns are not linearly independent and thus it is not full rank. What does this tell us about $\mathbf{X}^\top \mathbf{X}$? Review your notes from Linear Algebra if needed :)
